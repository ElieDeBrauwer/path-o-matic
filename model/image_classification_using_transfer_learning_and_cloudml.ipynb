{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying your own images using transfer learning and Google Cloud ML Engine\n",
    "---\n",
    "## Introduction\n",
    "This notebook can be used to classify a new dataset of images using *transfer learning* based on *Google Cloud Machine Learning Engine*.\n",
    "\n",
    "It is based on the following github repo: https://github.com/amygdala/tensorflow-workshop.git\n",
    "\n",
    "The notebook is intended to be executed from inside the *__tensorflow-workshop/workshop_sections/transfer_learning/cloudml/__* directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_name = \"pathomatic\"\n",
    "user_name = \"bardi\"\n",
    "model_version = \"v1\"\n",
    "train_on_cloud = False\n",
    "skip_preproc = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper function for printing out streaming subprocess output\n",
    "import subprocess\n",
    "import sys\n",
    "def exec_subprocess(cmd):\n",
    "  proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n",
    "  while proc.poll() is None:\n",
    "    line = proc.stdout.readline()\n",
    "    sys.stdout.write(line)\n",
    "  # Might still be data on stdout at this point. Grab any remainder.\n",
    "  for line in proc.stdout.read().split('\\n'):\n",
    "    sys.stdout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the Project ID\n",
    "project_id_rd = !gcloud config list project --format \"value(core.project)\"\n",
    "project_id = project_id_rd.fields()[0][0]\n",
    "print (\"Project ID: %s\" % project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the Google Storage bucket\n",
    "bucket = \"gs://%s-%s-ml\" % (project_id, project_name)\n",
    "print (\"Bucket name: %s\" % bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a timestemp for the pre-processing JOB ID\n",
    "# Note that DataFlow doesn't like underscores\n",
    "timestamp_preproc = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(\"Time stamp: %s\" % timestamp_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute the pre-processing\n",
    "if not skip_preproc:\n",
    "  exec_subprocess(\"chmod a+x ./%s_preproc.sh\" % project_name)\n",
    "  exec_subprocess(\"USER=%s DATE=%s ./%s_preproc.sh %s\" % (user_name, timestamp_preproc, project_name, bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define pre-processing data output path\n",
    "if skip_preproc:\n",
    "  gcs_path_preproc = \"gs://asl_project/preproc\"\n",
    "else:\n",
    "  gcs_path_preproc = \"%s/%s/preproc/%s\" % (bucket, user_name, timestamp_preproc)\n",
    "print (\"Google Cloud Storage pre-processing path: %s\" % gcs_path_preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define training Job ID\n",
    "timestamp_training = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "job_id=(\"%s_%s_%s\" % (project_name, user_name, timestamp_training)).replace('-', \"_\")\n",
    "print (\"Job ID: %s\" % job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define training path\n",
    "gcs_path_train = \"%s/%s/train/%s\" % (bucket, user_name, timestamp_training)\n",
    "print (\"Google Cloud Storage training path: %s\" % gcs_path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the training on CLOUD\n",
    "# =========================\n",
    "#\n",
    "# This script will output summary and model checkpoint information under <gcs_path>/training\n",
    "#\n",
    "# If --package-path /my/code/path/trainer is specified and there is a setup.py file \n",
    "# at /my/code/path/setup.py then that file will be invoked with sdist and the generated tar files\n",
    "# will be uploaded to Cloud Storage. Otherwise a temporary setup.py file will be generated for the build.\n",
    "#\n",
    "# See https://cloud.google.com/sdk/gcloud/reference/ml-engine/jobs/submit/training\n",
    "#\n",
    "# The scale-tier story:\n",
    "# > Options are: BASIC, BASIC_GPU, STANDARD_1, PREMIUM_1 or CUSTOM\n",
    "# > By default there are 25 ML units available. A PREMIUM_1 scale-tier however requires 75 ML units.\n",
    "# > To speed-up training we've requested an upgrade to 100 ML units\n",
    "# > using the 'Cloud Machine Learning Engine Quota Request form' on https://cloud.google.com/ml-engine/quotas\n",
    "#\n",
    "# Currently unused flags:\n",
    "# --config=CONFIG\n",
    "# > Path to the job configuration file. The file should be a YAML document (JSON also accepted)\n",
    "# > containing a Job resource as defined in the API (all fields are optional)\n",
    "# > https://cloud.google.com/ml/reference/rest/v1/projects.jobs\n",
    "# > If an option is specified both in the configuration file and via command line arguments,\n",
    "# > the command line arguments override the configuration file.\n",
    "#\n",
    "# --job-dir=JOB_DIR\n",
    "# > A Google Cloud Storage path in which to store training outputs and other data needed for training.\n",
    "# > This path will be passed to your TensorFlow program as --job_dir command-line arg.\n",
    "# > The benefit of specifying this field is that Cloud ML Engine will validate the path for use in training.\n",
    "# > If packages must be uploaded and --staging-bucket is not provided, this path will be used instead.\n",
    "#\n",
    "# --packages=[PACKAGE,â€¦]\n",
    "# > Path to Python archives used for training. These can be local paths (absolute or relative),\n",
    "# > in which case they will be uploaded to the Cloud Storage bucket given by --staging-bucket,\n",
    "# > or Cloud Storage URLs (gs://bucket-name/path/to/package.tar.gz).\n",
    "#\n",
    "# --staging-bucket=STAGING_BUCKET\n",
    "# > Bucket in which to stage training archives.\n",
    "# > Required only if a file upload is necessary (that is, other flags include local paths)\n",
    "# > and no other flags implicitly specify an upload path.\n",
    "#\n",
    "# > --stream-logs\n",
    "# > Block until job completion and stream the logs while the job runs.\n",
    "# > Note that even if command execution is halted, the job will still run until cancelled with\n",
    "if train_on_cloud:\n",
    "  exec_subprocess(\"gcloud ml-engine jobs submit training %s\" % job_id + \\\n",
    "    \" --module-name trainer.task\" + \\\n",
    "    \" --package-path trainer\" + \\\n",
    "    \" --staging-bucket %s\" % bucket + \\\n",
    "    \" --region us-central1\" + \\\n",
    "    \" --runtime-version=1.2\" + \\\n",
    "    \" --scale-tier=PREMIUM_1\" + \\\n",
    "    \" --\" + \\\n",
    "    \" --output_path %s\" % (gcs_path_train + \"/training\") + \\\n",
    "    \" --eval_data_paths %s\" % (gcs_path_preproc + \"/eval*\") + \\\n",
    "    \" --train_data_paths %s\" % (gcs_path_preproc + \"/train*\") + \\\n",
    "    \" --eval_set_size 474\" + \\\n",
    "    \" --eval_batch_size 25\" + \\\n",
    "    \" --classifier_label_count 2\" + \\\n",
    "    \" --max_steps 100000\" + \\\n",
    "    \" --model_type baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the training locally\n",
    "# ========================\n",
    "#\n",
    "# Note that max_steps is configured much lower.\n",
    "# This is because local training is typically used for initial checks.\n",
    "# Once local training is working, we can switch to cloud training\n",
    "if not train_on_cloud:\n",
    "  exec_subprocess(\"gcloud ml-engine local train\" + \\\n",
    "    \" --module-name trainer.task\" + \\\n",
    "    \" --package-path trainer\" + \\\n",
    "    \" --\" + \\\n",
    "    \" --output_path %s\" % (gcs_path_train + \"/training\") + \\\n",
    "    \" --eval_data_paths %s\" % (gcs_path_preproc + \"/eval*\") + \\\n",
    "    \" --train_data_paths %s\" % (gcs_path_preproc + \"/train*\") + \\\n",
    "    \" --eval_set_size 474\" + \\\n",
    "    \" --eval_batch_size 25\" + \\\n",
    "    \" --classifier_label_count 2\" + \\\n",
    "    \" --max_steps 10\" + \\\n",
    "    \" --model_type baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Monitor the training\n",
    "exec_subprocess(\"gcloud ml-engine jobs stream-logs %s\" % (job_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See the results in TensorBoard\n",
    "from google.datalab.ml import TensorBoard\n",
    "pid = TensorBoard.start(\"%s/training\" % gcs_path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See the running TensorBoard's\n",
    "TensorBoard.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell to stop the previously started TensorBoard process\n",
    "TensorBoard.stop(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "# This will give an error if the model already exists, but this is expected and OK.\n",
    "exec_subprocess(\"chmod a+x ./model.sh\")\n",
    "exec_subprocess(\"./model.sh %s %s %s\" % (gcs_path_train, model_version, project_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a list of deployed models\n",
    "!gcloud ml-engine models list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare prediction request.json\n",
    "#!python images_to_json.py -o request.json ./prediction_images/hedgehog.jpg ./prediction_images/puppy1.jpg ./prediction_images/puppy2.jpg -t='JPEG'\n",
    "!gsutil cp gs://qwiklabs-gcp-101a7711606c713e-hugs-ml/patient_004_node_4_00001_12597_88778_low.png ./prediction_images\n",
    "!python images_to_json.py -o request.json ./prediction_images/patient_004_node_4_00001_12597_88778_low.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run predictions on a number of images\n",
    "exec_subprocess(\"gcloud ml-engine predict --model %s --json-instances request.json \" % (project_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "hiddenCell": false
   },
   "outputs": [],
   "source": [
    "# If needed, run the following to update gcloud\n",
    "#!yes | gcloud components update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
